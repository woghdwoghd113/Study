{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## 1. 유사도\n","\n","pororo를 안쓰신다기에, sklearn 벡터라이저를 활용하여 전처리를 했습니다.\n","자카드, 유클리디안, 멘허튼으로 구성해봤습니다."],"metadata":{"id":"OfmvMN-Gc-IC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4EzRxkQZQ6w"},"outputs":[],"source":["sentence = ( \"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\",\n","\t     \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.\")\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# 객체 생성\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# 문장 벡터화 진행 - 이것을 쓸겁니다.\n","tfidf_matrix = tfidf_vectorizer.fit_transform(sentence)\n","\n","# 각 단어\n","text = tfidf_vectorizer.get_feature_names()\n","\n","# 각 단어의 벡터 값\n","idf = tfidf_vectorizer.idf_\n","idf\n","\n","print(\"첫번째 문장\",tfidf_matrix[0:1])\n","print(\"두번째 문장\",tfidf_matrix[1:2])"]},{"cell_type":"code","source":["# 1.유클리디안 유사도\n","from sklearn.metrics.pairwise import euclidean_distances\n","import numpy as np\n","\n","def l1_normalize(v): # 일반화\n","    norm = np.sum(v)\n","    return v / norm\n","    \n","# 벡터라이징한 2개의 문장 전부 노말라이즈.\n","tfidf_norm_l1 = l1_normalize(tfidf_matrix) \n","\n","euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"],"metadata":{"id":"oRRndp-ufOof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. 맨하탄 유사도\n","from sklearn.metrics.pairwise import manhattan_distances\n","\n","import numpy as np\n","\n","def l1_normalize(v):\n","    norm = np.sum(v)\n","    return v / norm \n","  \n","# L1 정규화  \n","tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n","\n","manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"],"metadata":{"id":"8plg9uVtfpGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. 자카드 유사도\n","\n","# 위에 sentence 문장 2개를 활용했습니다.\n","\n","data1 = sentence[0].split()\n","data2 = sentence[1].split()\n","data1\n","intd = set(data1) & set(data2) # 교집합\n","intd\n","un = set(data1) | set(data2) # 합집합\n","un\n","len(intd)/len(un) # data1과 dat2의 자카드 유사도 : 0.7"],"metadata":{"id":"nlR79p6ygGwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Text classification\n","데이터셋이 너무 크면, 기간내에 전달 못할 것 같아서 네이버 댓글 감성분석으로 구성해봤습니다.\n","\n","\n","- 데이터 설명\n","\n","네이버 영화 리뷰 데이터\n","총 200,000개 리뷰로 구성된 데이터로 영화 리뷰에 대한 텍스트와 해당 리뷰가 긍정인 경우 1, 부정인 경우 0을 표시한 레이블"],"metadata":{"id":"FYTcXXaWgy0m"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"0_HbM9AkhH_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","from konlpy.tag import Okt\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"3IegSZuSgxTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"],"metadata":{"id":"9lGwtFW0g_D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_table('ratings_train.txt')\n","test_data = pd.read_table('ratings_test.txt')"],"metadata":{"id":"hRK_vFedhFii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[:5] # 상위 5개 출력"],"metadata":{"id":"Sc4RsA3-hd5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# document 열과 label 열의 중복을 제외한 값의 개수\n","train_data['document'].nunique(), train_data['label'].nunique()"],"metadata":{"id":"nuMh8sZUhftN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# document 열의 중복 제거\n","train_data.drop_duplicates(subset=['document'], inplace=True)"],"metadata":{"id":"bCLO6c2Phh3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_data에서 해당 리뷰의 긍, 부정 유무 갯수\n","train_data['label'].value_counts().plot(kind = 'bar')\n"],"metadata":{"id":"2jdCGfvAhknJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터에 널값이 존재합니다\n","print(train_data.isnull().sum())"],"metadata":{"id":"rZneOhpvhpPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n","print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"],"metadata":{"id":"zY5oqKCjhvRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 한글과 공백을 제외하고 모두 제거\n","train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","train_data[:5]"],"metadata":{"id":"Akf1BZ9Ph1Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 네이버 영화 리뷰는 한글이 아니더라도 영어, 숫자, 특수문자로도 리뷰를 업로드할 수 있습니다. \n","# 다시 말해 기존에 한글이 없는 리뷰였다면 더 이상 아무런 값도 없는 빈(empty) 값이 되었을 것입니다. \n","# train_data에 공백(whitespace)만 있거나 빈 값을 가진 행이 있다면 Null 값으로 변경하도록 하고, Null 값이 존재하는지 확인해보겠습니다.\n","\n","train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n","train_data['document'].replace('', np.nan, inplace=True)\n","print(train_data.isnull().sum())"],"metadata":{"id":"zAkXQ-W-h5mP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 제거해줍니다.\n","train_data = train_data.dropna(how = 'any')\n","print(len(train_data))"],"metadata":{"id":"amJ9vDjoiJZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터에 앞서 진행한 전처리 과정을 동일하게 진행합니다.\n","\n","test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n","test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n","test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거\n","print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"],"metadata":{"id":"OlWtavKeiPnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰화를 위한 형태소 분석기는 KoNLPy의 Okt를 사용합니다.\n","\n","X_train = []\n","\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","okt = Okt()\n","for sentence in tqdm(train_data['document']):\n","    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n","    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n","    X_train.append(stopwords_removed_sentence)"],"metadata":{"id":"lDcdNMXQiVTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  테스트 데이터에 대해서도 동일하게 토큰화를 해줍니다.\n","\n","X_test = []\n","for sentence in tqdm(test_data['document']):\n","    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n","    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n","    X_test.append(stopwords_removed_sentence)"],"metadata":{"id":"c1vaMBwhii13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)"],"metadata":{"id":"feTilMu1oyH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여\n","print(tokenizer.word_index)"],"metadata":{"id":"Sl4Eu-Bio9kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n","threshold = 3\n","total_cnt = len(tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"],"metadata":{"id":"_CNtj_RCpAMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 등장 빈도가 threshold 값인 3회 미만. \n","# 즉, 2회 이하인 단어들은 단어 집합에서 무려 절반 이상을 차지합니다. \n","# 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 1.87%밖에 되지 않습니다. \n","# 아무래도 등장 빈도가 2회 이하인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 합니다. \n","# 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다.\n","\n","# 등장 빈도수가 2이하인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠습니다.\n","\n","# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n","# 0번 패딩 토큰을 고려하여 + 1\n","vocab_size = total_cnt - rare_cnt + 1\n","print('단어 집합의 크기 :',vocab_size)"],"metadata":{"id":"1iNdHFVxpE0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 집합의 크기는 19,416개입니다. 이를 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환합니다.\n","\n","tokenizer = Tokenizer(vocab_size) \n","tokenizer.fit_on_texts(X_train)\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"],"metadata":{"id":"kIcksa-Bpd9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_data에서 y_train과 y_test를 별도로 저장해줍니다.\n","\n","y_train = np.array(train_data['label'])\n","y_test = np.array(test_data['label'])"],"metadata":{"id":"IM3GK9H7pnLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 빈 샘플(empty samples) 제거\n","\n","# 전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 \n","# 구성되었던 샘플들은 빈(empty) 샘플이 되었다는 것을 의미합니다. \n","# 빈 샘플들은 어떤 레이블이 붙어있던 의미가 없으므로 빈 샘플들을 제거해주는 작업을 하겠습니다.\n","# 각 샘플들의 길이를 확인해서 길이가 0인 샘플들의 인덱스를 받아오겠습니다.\n","\n","drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n","\n","# 빈 샘플들을 제거\n","X_train = np.delete(X_train, drop_train, axis=0)\n","y_train = np.delete(y_train, drop_train, axis=0)\n","\n","# 빈 샘플들을 제거한 후의 샘플 개수\n","print(len(X_train))\n","print(len(y_train))"],"metadata":{"id":"MFBu5zA0pptG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다.\n","# 전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다.\n","\n","print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n","plt.hist([len(review) for review in X_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"UUgSBf0sp7d8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가장 긴 리뷰의 길이는 69이며, \n","# 그래프를 봤을 때 전체 데이터의 길이 분포는 대체적으로 약 11내외의 길이를 가지는 것을 볼 수 있습니다. \n","# 모델이 처리할 수 있도록 X_train과 X_test의 모든 샘플의 길이를 특정 길이로 동일하게 맞춰줄 필요가 있습니다. \n","# 특정 길이 변수를 max_len으로 정합니다. \n","# 대부분의 리뷰가 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값은 몇일까요?\n","# 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수를 만듭니다.\n","\n","def below_threshold_len(max_len, nested_list):\n","  count = 0\n","  for sentence in nested_list:\n","    if(len(sentence) <= max_len):\n","        count = count + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"],"metadata":{"id":"-9nGzBHNqAEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위의 분포 그래프를 봤을 때, max_len = 30이 적당할 것 같습니다.\n","max_len = 30\n","below_threshold_len(max_len, X_train)\n"],"metadata":{"id":"-g1d1yFeqKn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 훈련 데이터 중 약 94%의 리뷰가 30이하의 길이를 가지는 것을 확인했습니다.\n","# 모든 샘플의 길이를 30으로 맞추겠습니다.\n","\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)"],"metadata":{"id":"1Z--HbhoqRa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 100\n","hidden_units = 128\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","model.add(Bidirectional(LSTM(hidden_units))) # Bidirectional LSTM을 사용\n","model.add(Dense(1, activation='sigmoid'))\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"],"metadata":{"id":"Pep48MWy84i1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 조기 종료 조건에 따라서 8 에포크에서 훈련이 멈췄습니다. \n","# 훈련이 다 되었다면 테스트 데이터에 대해서 정확도를 측정할 차례입니다. \n","# 훈련 과정에서 검증 데이터의 정확도가 가장 높았을 때 저장된 모델인 'best_model.h5'를 로드합니다.\n","\n","loaded_model = load_model('best_model.h5')\n","print(\"\\n 테스트 정확도(accuracy): %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"metadata":{"id":"4Yl8O58Oqm59"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict\n","y_prediction = loaded_model.predict(X_test)\n","\n","# 이진화\n","pred_class = np.where(y_prediction > 0.5, 1 , 0)\n","\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","import matplotlib.pyplot as plt\n","\n","cm = confusion_matrix(y_true=y_test, y_pred=pred_class)"],"metadata":{"id":"UO86jK9R7upa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confusion matrix 결과\n","cm"],"metadata":{"id":"SbMgEb6rGids"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test 코드입니다.\n","\n","def sentiment_predict(new_sentence):\n","    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n","    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n","    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n","    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n","    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n","    score = float(loaded_model.predict(pad_new)) # 예측\n","    if(score > 0.5):\n","        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n","    else:\n","        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n","\n","\n","sentiment_predict('너무 재미없었다. 다시는 안보겠다.')"],"metadata":{"id":"rN_1U5v1q6p2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Text Summarization \n","\n","Transformer 기반 모델인 Text-to-Text Transfer Transformer (T5)를 사용할 예정입니다. T5는 encoder-decoder 구조를 갖는 모델로 요약, 번역등 다양한 task에서 활용됩니다."],"metadata":{"id":"irVZd0HOrHAm"}},{"cell_type":"code","source":["# 필요한 라이브러리들 설치\n","!pip install transformers==4.20.0\n","!pip install nltk\n","!pip install -U nltk\n","!pip install rouge-score\n","!pip install keras_nlp==0.3.0\n","!pip install datasets\n","!pip install huggingface-hub"],"metadata":{"id":"6k5aray_rKUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","from pprint import pprint\n","import json\n","import nltk\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","# 에러 메세지만 로깅\n","tf.get_logger().setLevel(logging.ERROR)\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"metadata":{"id":"ALBoIlYP-W8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요한 옵션들 셋팅\n","\n","# train과 test 데이터셋으로 나누는 비율\n","TRAIN_TEST_SPLIT = 0.1\n","\n","MAX_INPUT_LENGTH = 512  # encoder에 들어갈 max input 길이\n","MIN_TARGET_LENGTH = 5  # decoder에 들어갈 min input 길이\n","MAX_TARGET_LENGTH = 128  # decoder에 들어갈 max input 길이\n","BATCH_SIZE = 8  # 모델 학습에 사용할 batch siz 크기\n","LEARNING_RATE = 2e-5  # 모델 학습에 사용할 learning rate\n","MAX_EPOCHS = 5  # 모델 학습에 사용할 epoch수\n","\n","# Hugging Face Model Hub로 부터 가져올 모델명\n","MODEL_CHECKPOINT = \"psyche/KoT5-summarization\"\n","     "],"metadata":{"id":"Nqvbnh48-bxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the dataset\n","\n","전체 데이터셋은 AI Hub에서 다운받으실 수 있습니다. \n","\n","https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=97\n","\n","우선 구글 드라이브에서 데이터 샘플을 가져옵니다.\n","\n","다운 받은 데이터셋을 Hugging Face Datasets 포멧으로 변환합니다.\n","\n","\n","abstractive가 요약문이고 article_original이 원본 본문입니다."],"metadata":{"id":"Mh2SVriG-eMh"}},{"cell_type":"code","source":["!gdown 1S5kUCc-u-F2w5JOgS81w2ht6Wmcjo7-y\n","!ls\n","!apt-get install jq\n","!head -n 1 ./sample.jsonl | jq '.'"],"metadata":{"id":"ymz5afQ_-ehi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터의 키 값을 article_original은 document로, abstractive는 summary로 변경합니다."],"metadata":{"id":"qLznvPxd-wTv"}},{"cell_type":"code","source":["# google drive에 있는 데이터를 사용하기 위해 접근\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"O8sfpBK12KUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","from datasets import Dataset, load_dataset\n","ds_path = \"./sample.jsonl\"\n","\n","# 데이터 담을 배열 선언\n","_id, document, summary = [], [], []\n","\n","# 제이슨 읽어오기\n","with open(ds_path, 'r') as f:\n","  while True:\n","    line = f.readlines()\n","    if not line: break\n","    doc = json.loads(line)\n","    _id.append(doc['id'])\n","    document.append(\" \".join(doc['article_original']))\n","    summary.append(doc['abstractive'])\n","\n","# raw_datasets = load_dataset(\"xsum\", split=\"train\")\n","raw_datasets = Dataset.from_dict({\"id\":_id,\n","                                  \"document\":document,\n","                                  \"summary\":summary})\n","print(raw_datasets[0])\n","\"\"\"\n","from datasets import Dataset\n","import json\n","ds_path = \"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/법률_train_original/train_original.json\"\n","\n","# 데이터 담을 배열 선언\n","_id, document, summary = [], [], []\n","\n","with open(ds_path, \"r\") as st_json:\n","    doc = json.load(st_json)\n","\n","for i in doc['documents']:\n","    _id.append(i['id'])\n","    document.append(' '.join([sent['sentence'] for sent in i['text'][0]]))\n","    summary.append(i['abstractive'][0])\n","\n","# raw_datasets = load_dataset(\"xsum\", split=\"train\")\n","raw_datasets = Dataset.from_dict({\"id\":_id,\n","                                  \"document\":document,\n","                                  \"summary\":summary})\n","\n","print(raw_datasets[0])"],"metadata":{"id":"mn19xoJoIDNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터를 학습용과 테스트용으로 스플릿합니다.\n","raw_datasets = raw_datasets.train_test_split(\n","    train_size=1-TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",")\n","raw_datasets"],"metadata":{"id":"7hgnSfx7-_4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hugging Face Transformers의 Tokenizer 라이브러리로 MODEL_CHECKPOINT에 저장되어 있는 토크나이저를 로딩합니다. \n","# Hugging Face Model Hub로 부터 가져올 토크나이징 모델명이 위에서 선언한 MODEL_CHECKPOINT, 즉 psyche/KoT5-summarization을 말합니다.\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"],"metadata":{"id":"tzK-FZCvA5Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# T5에서는 task를 prefix로 명시해주는 경우가 있습니다. 요약이라는 prefix를 추가해줍니다.\n","if MODEL_CHECKPOINT in [\"psyche/KoT5-summarization\", \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n","    prefix = \"요약: \"\n","else:\n","    prefix = \"\"\n","\n","prefix"],"metadata":{"id":"l5klRBOiA7g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 전처리를 위한 함수를 선언합니다.\n","# 전처리 함수에서는 prefix 추가 및 tokenization을 진행합니다.\n","# 이 과정에서 token_tpye_ids, attention_mask도 자동으로 생성됩니다.\n","\n","def preprocess_function(examples):\n","    inputs = [prefix + doc for doc in examples[\"document\"]]\n","    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n","\n","    # Setup the tokenizer for targets\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n","        )\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n"],"metadata":{"id":"uHINk31AA9ok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이전에 생성했던 dataset 오브젝트에 map 메소드를 통해 preprocess_function 함수를 적용합니다.\n","# map을 사용할 경우 dataset내에 쪼개져서 구성되어 있는 train, validation, test 데이터셋에도 한번에 적용됩니다.\n","\n","tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"],"metadata":{"id":"SRrsU2ukA_MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 요약 문제는 sequence-to-sequence task이기 때문에 TFAutoModelForSeq2SeqLM 클래스를 통해 T5 모델을 로딩합니다.\n","# tokenizer와 마찬가지로 from_pretrained 메소드를 통해 T5 모델을 다운받을 수 있습니다.\n","\n","from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, from_pt=True) # tf_model.h5 포멧의 경우는 from_pt=True 옵션 없어도 됨"],"metadata":{"id":"k8_BU-NuBBdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequencce to Sequence 모델을 학습하기 위해서는 이에 맞는 data collator가 필요합니다.\n","# input뿐만 아니라 labels에 대해서도 padding처리를 해줘야 하기 때문입니다. 이를 위해 DataCollatorForSeq2Seq 를 사용할 수 있습니다.\n","# keras를 이용하고 있으므로 리턴 타입 tf.Tensor로 얻을 수 있도록 return_tensors=\"tf\" 을 셋팅해줍니다.\n","\n","from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"],"metadata":{"id":"u8_oVNu9BDG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data_collator function을 적용해줍니다.\n","# 추가적으로 generation_dataset 이라는 작은 사이즈의 데이터셋을 학습시에 계산할 ROUGE score를 위해 추가로 생성해줍니다.\n","\n","train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","generation_dataset = (\n","    tokenized_datasets[\"test\"]\n","    .shuffle()\n","    .select(list(range(200)))\n","    .to_tf_dataset(\n","        batch_size=BATCH_SIZE,\n","        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","        shuffle=False,\n","        collate_fn=data_collator,\n","    )\n",")"],"metadata":{"id":"pVcEykmpBI2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이제 optimizer를 정의하고 모델을 컴파일 해줍니다. loss 계산은 내부적으로 처리됩니다.\n","\n","optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","model.compile(optimizer=optimizer)"],"metadata":{"id":"W48Y-_2KBL3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습을 진행하는 동안 동시에 모델을 평가하기 위해 \n","# metric_fn을 정의해서 ground-truth와 prediction 간의 ROUGE score를 계산해보겠습니다.\n","\n","import keras_nlp\n","\n","rouge_l = keras_nlp.metrics.RougeL()\n","\n","\n","def metric_fn(eval_predictions):\n","    predictions, labels = eval_predictions\n","    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    for label in labels:\n","        label[label < 0] = tokenizer.pad_token_id  # masked label tokens 교체\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    result = rouge_l(decoded_labels, decoded_predictions)\n","    # 다른 metrics도 추가 가능하지만, 여기에서는 f1_score만 출력되도록 셋팅\n","    result = {\"RougeL\": result[\"f1_score\"]}\n","\n","    return result"],"metadata":{"id":"AmLBtpQCBLxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습을 시작하는 부분입니다.\n","\n","from transformers.keras_callbacks import KerasMetricCallback\n","\n","metric_callback = KerasMetricCallback(\n","    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",")\n","\n","callbacks = [metric_callback]\n","\n","# test set을 validation set으로 사용해보겠습니다\n","model.fit(\n","    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n",")\n","     "],"metadata":{"id":"4mTz6FCEBPPo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inference 부분입니다.\n","# 학습한 모델로 inference를 해보겠습니다. \n","# Huggingface의 pipeline 메소드에서 제공하는 summarization을 사용하면 쉽게 구현할 수 있습니다.\n","# pipeline 메소드에 학습한 모델과 토크나이저를 인자로 넣어줍니다. \n","# 모델이 TF로 학습되었으므로 framework=\"tf\"도 함께 인자로 넣어줍니다.\n","\n","from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","\n","summary = summarizer(\n","              raw_datasets[\"test\"][0][\"document\"],\n","              min_length=MIN_TARGET_LENGTH,\n","              max_length=MAX_TARGET_LENGTH,\n","          )\n","\n","print(f'document:{raw_datasets[\"test\"][0][\"document\"]}')\n","print(f'label summary:{raw_datasets[\"test\"][0][\"summary\"]}')\n","print(f'pred summary: {summary[0][\"summary_text\"]}')"],"metadata":{"id":"iRlJ9cGYBQku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[raw_datasets[\"test\"][0][\"summary\"].split()]"],"metadata":{"id":"GhIgZHJKck6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary[0][\"summary_text\"].split()"],"metadata":{"id":"ilvehGiHcnay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk.translate.bleu_score as bleu\n","print('BLEU Score:',bleu.sentence_bleu([raw_datasets[\"test\"][0][\"summary\"].split()],summary[0][\"summary_text\"].split()))"],"metadata":{"id":"hHq_zo0SBSBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","slist = []\n","for i in range(10):\n","    slist.append(input(\"input string:\"))\n","\n","for i in range(10):\n","    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","\n","    summary = summarizer(\n","                slist[i],\n","                min_length=MIN_TARGET_LENGTH,\n","                max_length=MAX_TARGET_LENGTH,\n","            )\n","    print(f'pred summary: {summary[0][\"summary_text\"]}')\n","\n","    slist = []\n","\"\"\"\n","\n","\n","## 여기서 문장 길게 쓰신 다음 결과 보시면 됩니다\n","sentence = input(\"input string:\"))\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","summary = summarizer(\n","            sentence,\n","            min_length=MIN_TARGET_LENGTH,\n","            max_length=MAX_TARGET_LENGTH,\n","        )\n","print(f'pred summary: {summary[0][\"summary_text\"]}')"],"metadata":{"id":"CDEtMQZBVzB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oGyHwbVhife_"},"execution_count":null,"outputs":[]}]}