{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","collapsed_sections":["irVZd0HOrHAm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["## Text Summarization \n","\n","Transformer 기반 모델인 Text-to-Text Transfer Transformer (T5)를 사용할 예정으로 T5는 encoder-decoder 구조를 갖는 모델로 요약, 번역등 다양한 task에서 활용되고 있음.\n","\n","데이터 셋은 aihub에서 제공하는 문서요약 텍스트로 원문 데이터 40만 건(신문기사 30만 건, 기고문 6만 건, 잡지기사 1만 건, 법원 판결문 3만 건)을 활용하여 각각 추출요약 40만 건, 생성요약 40만 건, 총 80만 건의 요약문 도출 / 원문으로부터 변형 없이 그대로 선택된 3개 문장으로 추출요약문 생성 / 원문의 내용을 바탕으로 재작성된 생성요약문 생성 "],"metadata":{"id":"irVZd0HOrHAm"}},{"cell_type":"code","source":["!pip install transformers==4.20.0\n","!pip install nltk\n","!pip install -U nltk\n","!pip install rouge-score\n","!pip install keras_nlp==0.3.0\n","!pip install datasets\n","!pip install huggingface-hub"],"metadata":{"id":"6k5aray_rKUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","from pprint import pprint\n","import json\n","import nltk\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","tf.get_logger().setLevel(logging.ERROR)\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"metadata":{"id":"ALBoIlYP-W8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_TEST_SPLIT = 0.1\n","\n","MAX_INPUT_LENGTH = 512 \n","MIN_TARGET_LENGTH = 5 \n","MAX_TARGET_LENGTH = 512\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5  \n","MAX_EPOCHS = 5\n","\n","MODEL_CHECKPOINT = \"psyche/KoT5-summarization\""],"metadata":{"id":"Nqvbnh48-bxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"O8sfpBK12KUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","import json\n","ds_path = \"/content/drive/MyDrive/article_train_original.json\"\n","\n","_id, document, summary = [], [], []\n","\n","with open(ds_path, \"r\") as st_json:\n","    doc = json.load(st_json)\n","\n","for i in doc['documents']:\n","    _id.append(i['id'])\n","    document.append(' '.join([sent['sentence'] for sent in i['text'][0]]))\n","    summary.append(i['abstractive'][0])\n","\n","raw_datasets = Dataset.from_dict({\"id\":_id,\n","                                  \"document\":document,\n","                                  \"summary\":summary})\n","\n","print(raw_datasets[0])"],"metadata":{"id":"mn19xoJoIDNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_datasets = raw_datasets.train_test_split(\n","    train_size=1-TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",")\n","raw_datasets"],"metadata":{"id":"7hgnSfx7-_4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"],"metadata":{"id":"tzK-FZCvA5Dk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if MODEL_CHECKPOINT in [\"psyche/KoT5-summarization\", \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n","    prefix = \"요약: \"\n","else:\n","    prefix = \"\"\n","\n","prefix"],"metadata":{"id":"l5klRBOiA7g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    inputs = [prefix + doc for doc in examples[\"document\"]]\n","    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n","        )\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs"],"metadata":{"id":"uHINk31AA9ok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"],"metadata":{"id":"SRrsU2ukA_MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, from_pt=True)"],"metadata":{"id":"k8_BU-NuBBdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"],"metadata":{"id":"u8_oVNu9BDG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n",")\n","test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n","    batch_size=BATCH_SIZE,\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n",")\n","generation_dataset = (\n","    tokenized_datasets[\"test\"]\n","    .shuffle()\n","    .select(list(range(200)))\n","    .to_tf_dataset(\n","        batch_size=BATCH_SIZE,\n","        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","        shuffle=False,\n","        collate_fn=data_collator,\n","    )\n",")"],"metadata":{"id":"pVcEykmpBI2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","model.compile(optimizer=optimizer)"],"metadata":{"id":"W48Y-_2KBL3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras_nlp\n","\n","rouge_l = keras_nlp.metrics.RougeL()\n","\n","\n","def metric_fn(eval_predictions):\n","    predictions, labels = eval_predictions\n","    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    for label in labels:\n","        label[label < 0] = tokenizer.pad_token_id \n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    result = rouge_l(decoded_labels, decoded_predictions)\n","  \n","    result = {\"RougeL\": result[\"f1_score\"]}\n","\n","    return result"],"metadata":{"id":"AmLBtpQCBLxW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers.keras_callbacks import KerasMetricCallback\n","\n","metric_callback = KerasMetricCallback(\n","    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",")\n","\n","callbacks = [metric_callback]\n","\n","model.fit(\n","    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n",")    "],"metadata":{"id":"6hy-NNljO7AT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","\n","summary = summarizer(\n","              raw_datasets[\"test\"][0][\"document\"],\n","              min_length=MIN_TARGET_LENGTH,\n","              max_length=MAX_TARGET_LENGTH,\n","          )\n","\n","print(f'document:{raw_datasets[\"test\"][0][\"document\"]}')\n","print(f'label summary:{raw_datasets[\"test\"][0][\"summary\"]}')\n","print(f'pred summary: {summary[0][\"summary_text\"]}')"],"metadata":{"id":"iRlJ9cGYBQku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[raw_datasets[\"test\"][0][\"summary\"].split()]"],"metadata":{"id":"GhIgZHJKck6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary[0][\"summary_text\"].split()"],"metadata":{"id":"ilvehGiHcnay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import nltk.translate.bleu_score as bleu\n","# print('BLEU Score:',bleu.sentence_bleu([raw_datasets[\"test\"][0][\"summary\"].split()],summary[0][\"summary_text\"].split()))"],"metadata":{"id":"hHq_zo0SBSBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = input(\"input string:\")\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","summary = summarizer(\n","            sentence,\n","            min_length=MIN_TARGET_LENGTH,\n","            max_length=MAX_TARGET_LENGTH,\n","        )\n","print(f'pred summary: {summary[0][\"summary_text\"]}')"],"metadata":{"id":"CDEtMQZBVzB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## original 및 variant 요약 결과 저장 "],"metadata":{"id":"5iQDkvF2lJo5"}},{"cell_type":"code","source":["import pandas as pd\n","original = pd.read_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/original.csv\",encoding='cp949',header=None)\n","original.columns = ['contents']\n","variant = pd.read_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/variant.csv\",encoding='cp949',header=None)\n","variant.columns = ['contents']"],"metadata":{"id":"6gNpVdIFlNbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n","summary_original = summarizer(\n","              original['contents'].values.tolist(),\n","              min_length=MIN_TARGET_LENGTH,\n","              max_length=MAX_TARGET_LENGTH,\n","          )\n","summary_variant = summarizer(\n","              variant['contents'].values.tolist(),\n","              min_length=MIN_TARGET_LENGTH,\n","              max_length=MAX_TARGET_LENGTH,\n","          )"],"metadata":{"id":"pkIvASawig10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_original = [i['summary_text'] for i in summary_original]\n","summary_original = pd.DataFrame(summary_original)\n","summary_original.columns = ['contents']\n","summary_original.to_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/summary_original.csv\", index=False)\n","summary_variant = [i['summary_text'] for i in summary_variant]\n","summary_variant = pd.DataFrame(summary_variant)\n","summary_variant.columns = ['contents']\n","summary_variant.to_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/summary_variant.csv\", index=False)"],"metadata":{"id":"k0dd3ycMm7sk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Original / variant 차이 비교"],"metadata":{"id":"BF4JcgzN8TY0"}},{"cell_type":"code","source":["summary_original = pd.read_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/summary_original.csv\")\n","summary_variant = pd.read_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/summary_variant.csv\")\n","\n","df_diff = []\n","for i,j in zip(summary_original['contents'], summary_variant['contents']):\n","    tmp_org = set(i.split(\" \"))\n","    tmp_var = set(j.split(\" \"))\n","    diff = list(tmp_org.union(tmp_var) - tmp_org.intersection(tmp_var))\n","    df_diff.append(diff)\n","\n","df_diff = pd.DataFrame({'difference':df_diff})\n","df_diff.to_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/difference.csv\", index=False)\n","df_diff"],"metadata":{"id":"9IWkuM2T8S7O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## mysummary와 original summary간의 rouge 계산"],"metadata":{"id":"4cr_CYQLsOIq"}},{"cell_type":"code","source":["!pip install rouge py-rouge\n","!pip install rouge-score"],"metadata":{"id":"RYI9PgnDuvxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from rouge import Rouge\n","from rouge_score import rouge_scorer\n","import numpy as np\n","\n","mysummary = pd.read_csv(\"/content/drive/MyDrive/숨고/이인균님(인물 분류, 문서 요약)/mysummary.csv\",encoding='cp949',header=None)\n","mysummary.columns = ['contents']\n","\n","rouge = Rouge()\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n","\n","rouge_1 = []\n","rouge_2 = []\n","rouge_lsum = []\n","rouge_u = []\n","rouge_su = []\n","rdass = []\n","for i,j in zip(mysummary['contents'], summary_original['contents']):\n","    rouge_scores = rouge.get_scores(i, j)\n","    rouge_u_scores = scorer.score(i, j)\n","    rouge_1.append(rouge_scores[0]['rouge-1'])\n","    rouge_2.append(rouge_scores[0]['rouge-2'])\n","    rouge_lsum.append(rouge_u_scores[0]['rougeLsum'][2])\n","    rouge_u.append(rouge_u_scores[0]['rouge1'][2])\n","    rouge_su.append(rouge_u_scores[0]['rouge2'][2])\n","    rdass.append((rouge_u_scores['rouge1'][2] + rouge_u_scores['rouge2'][2]) / 2)\n","\n","# 결과 출력\n","print(\"rouge-1: \", np.mean(rouge_1))\n","print(\"rouge-2: \", np.mean(rouge_2))\n","print(\"rouge-lsum: \", np.mean(rouge_lsum))\n","print(\"rouge-u: \", np.mean(rouge_u))\n","print(\"rouge-su: \", np.mean(rouge_su))\n","print(\"rdass: \", np.mean(rdass))"],"metadata":{"id":"1wzubiYNnzrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cOnqR7Kq3WMO"},"execution_count":null,"outputs":[]}]}