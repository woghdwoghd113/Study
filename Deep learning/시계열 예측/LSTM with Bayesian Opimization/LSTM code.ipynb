{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOlaLZVijN8sl2eHElOTtml"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## google drive mount"],"metadata":{"id":"GnwteTUeIf8b"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TLJz_N22Iah_","executionInfo":{"status":"ok","timestamp":1703863073361,"user_tz":-540,"elapsed":2303,"user":{"displayName":"민재홍","userId":"10669517274837738962"}},"outputId":"5a84bd18-1e78-4a4a-f646-b27476764507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## library import"],"metadata":{"id":"vLv0YxOxOT2w"}},{"cell_type":"code","source":["# bayesian optimization 설치\n","!pip install hyperopt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JepikI18nbdH","executionInfo":{"status":"ok","timestamp":1703860819952,"user_tz":-540,"elapsed":15560,"user":{"displayName":"민재홍","userId":"10669517274837738962"}},"outputId":"edad71ab-3287-4b06-c9e0-8efca126a543"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (0.2.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.11.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt) (1.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt) (3.2.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.18.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt) (4.66.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt) (0.10.9.7)\n"]}]},{"cell_type":"code","source":["import numpy as np, os, pandas as pd, sys\n","from hyperopt import fmin, hp, tpe\n","from keras import backend as K\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Dense, LSTM, BatchNormalization\n","from keras.models import save_model, Sequential\n","from keras.optimizers import AdamW\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import shuffle"],"metadata":{"id":"F-i347SVOW3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Constants"],"metadata":{"id":"cV_1X0WqdlC5"}},{"cell_type":"code","source":["# 데이터 경로, 변수 등 수정하여 사용할 변수(하이퍼파라미터는 아래 bayesian에서 설정)\n","\n","DATA_PATH = \"/content/drive/MyDrive/숨고/신유진님(LSTM bayesian optimization)/data/\"\n","MODEL_PATH = \"/content/drive/MyDrive/숨고/신유진님(LSTM bayesian optimization)/result/model/\"\n","GRAPH_PATH = \"/content/drive/MyDrive/숨고/신유진님(LSTM bayesian optimization)/result/graph/\"\n","METRIC_RESULT_PATH = \"/content/drive/MyDrive/숨고/신유진님(LSTM bayesian optimization)/result/metric/\"\n","INPUT_COLUMN = [\"TEMP_AVG\",\"PRECI_DAY\",\"POP\",\"BUSINESS\",\"RATIO\",\"PRICE\"]\n","TARGET_COLUMN = \"HOUSING_E\"\n","TRAIN_RATE = 0.8\n","VALIDATION_RATE=0.1\n","SEQ_LEN = 6\n","EPOCHS = 300\n","PATIENCE = 30\n","MAX_EVALS = 3"],"metadata":{"id":"ihPSxQhXdsph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## data load"],"metadata":{"id":"i3Hlh0h1OoAQ"}},{"cell_type":"code","source":["# 불러온 DATA_PATH로 데이터 목록 저장\n","data_list = os.listdir(DATA_PATH)\n","print(\"data_list :\", data_list)\n","\n","# data명을 key, data를 value로 하는 딕셔너리 생성\n","data_dict = {}\n","for data_name in data_list:\n","    data_dict[data_name.split(\".\")[0]] = pd.read_csv(DATA_PATH+data_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtcLx0csI3Gr","executionInfo":{"status":"ok","timestamp":1703863425246,"user_tz":-540,"elapsed":385,"user":{"displayName":"민재홍","userId":"10669517274837738962"}},"outputId":"bfae8859-ba0f-4ad2-afd2-a7164f7ea3cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data_list : ['SIG_CD_28110.csv']\n"]}]},{"cell_type":"code","source":["data_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUztGwSoO8j6","executionInfo":{"status":"ok","timestamp":1703863425766,"user_tz":-540,"elapsed":3,"user":{"displayName":"민재홍","userId":"10669517274837738962"}},"outputId":"855262b6-a094-4727-ec33-1b97238c1efd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'SIG_CD_28110':      SIG_CD     SD SGG    TIME  YEAR  MONTH   HOUSING_E   TEMP_AVG  PRECI_DAY  \\\n"," 0     28110  인천광역시  중구  200801  2008      1   9201.7842  -2.207293   0.240370   \n"," 1     28110  인천광역시  중구  200802  2008      2   9130.5615  -1.462737   0.111990   \n"," 2     28110  인천광역시  중구  200803  2008      3   8173.5771   5.983078   1.191049   \n"," 3     28110  인천광역시  중구  200804  2008      4   8241.7100  11.961664   1.363898   \n"," 4     28110  인천광역시  중구  200805  2008      5   7726.3359  15.747392   1.561367   \n"," ..      ...    ...  ..     ...   ...    ...         ...        ...        ...   \n"," 151   28110  인천광역시  중구  202008  2020      8  21723.7950  25.259909  10.486750   \n"," 152   28110  인천광역시  중구  202009  2020      9  22150.0310  20.747995   6.011920   \n"," 153   28110  인천광역시  중구  202010  2020     10  16869.8730  14.210952   0.046633   \n"," 154   28110  인천광역시  중구  202011  2020     11  17771.9320   7.926172   1.619568   \n"," 155   28110  인천광역시  중구  202012  2020     12  19156.7790   0.164155   0.143381   \n"," \n","         POP  BUSINESS      RATIO   PRICE  \n"," 0     89333       370  15.315058   93.72  \n"," 1     89053       370  15.315058   94.80  \n"," 2     88813       370  15.315058   87.64  \n"," 3     88540       370  15.315058   89.80  \n"," 4     88484       370  15.315058   91.60  \n"," ..      ...       ...        ...     ...  \n"," 151  139154       745  22.304438  104.13  \n"," 152  139299       745  22.304438  116.36  \n"," 153  139391       745  22.304438  102.52  \n"," 154  139646       745  22.304438  105.89  \n"," 155  139729       745  22.304438  110.38  \n"," \n"," [156 rows x 13 columns]}"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## data preprocess\n","- 아래 과정을 모든 행정구역 데이터에 대해 반복\n","    - train/validation/test 분리\n","    - minmaxscaling 수행\n","    - lstm 학습을 위한 전처리"],"metadata":{"id":"t36CvFSQUYSr"}},{"cell_type":"code","source":["# lstm에 맞게 데이터 처리하는 함수\n","def sequential_processing(data, window_size, input_columns, target_column):\n","    X = data[INPUT_COLUMN]\n","    y = data[[TARGET_COLUMN]]\n","\n","    input = []\n","    output = []\n","    output_time = []\n","\n","    for i in range(len(data) - window_size):\n","        input.append(X.iloc[i:i+window_size])\n","        output.append(y.iloc[i+window_size])\n","        output_time.append(data.iloc[i+window_size,3:4])\n","\n","    return input, output, output_time\n","\n","data_preprocessed_dict = {}\n","sub_data_dict = {}\n","#  및 lstm에 맞게 전처리 수행 후, train/validation/test 분리 및 스케일링\n","for key,value in data_dict.items():\n","\n","    # sequential 전처리\n","    X_data_list, y_data_list, y_time_list = sequential_processing(value, SEQ_LEN, INPUT_COLUMN, TARGET_COLUMN)\n","\n","    # train/test 분리\n","    X_train_list = X_data_list[:round(len(X_data_list)*TRAIN_RATE)]\n","    y_train_list = y_data_list[:round(len(y_data_list)*TRAIN_RATE)]\n","    y_train_time_list = y_time_list[:round(len(y_time_list)*TRAIN_RATE)]\n","    X_test_list = X_data_list[round(len(X_data_list)*TRAIN_RATE):]\n","    y_test_list = y_data_list[round(len(y_data_list)*TRAIN_RATE):]\n","    y_test_time_list = y_time_list[round(len(y_time_list)*TRAIN_RATE):]\n","\n","    # validation의 경우 train set에서 VALIDATION_RATE 만큼 추출\n","    X_validation_list = X_train_list[round(len(X_train_list)*(1-VALIDATION_RATE)):]\n","    y_validation_list = y_train_list[round(len(y_train_list)*(1-VALIDATION_RATE)):]\n","    y_validation_time_list = y_train_time_list[round(len(y_train_time_list)*(1-VALIDATION_RATE)):]\n","    X_train_list = X_train_list[:round(len(X_train_list)*(1-VALIDATION_RATE))]\n","    y_train_list = y_train_list[:round(len(y_train_list)*(1-VALIDATION_RATE))]\n","    y_train_time_list = y_train_time_list[:round(len(y_train_time_list)*(1-VALIDATION_RATE))]\n","\n","    # train set의 input column들에 대해 minmaxscaling 수행 후, validation, test는 transform 진행\n","    # train list를 행방향 병합한 다음, minmaxscaler를 만들고 train, validation, test 리스트의 각 데이터프레임에 대해 transform 수행\n","    tmp_X_train_df = pd.concat(X_train_list,axis=0)\n","    mm_scaler = MinMaxScaler()\n","    mm_scaler = mm_scaler.fit(tmp_X_train_df)\n","\n","    # train, validation, test 리스트의 각 데이터프레임에 대해 transform 수행\n","    for i in range(len(X_train_list)):\n","        X_train_list[i] = pd.DataFrame(mm_scaler.transform(X_train_list[i]), columns=INPUT_COLUMN)\n","\n","    for i in range(len(X_validation_list)):\n","        X_validation_list[i] = pd.DataFrame(mm_scaler.transform(X_validation_list[i]), columns=INPUT_COLUMN)\n","\n","    for i in range(len(X_test_list)):\n","        X_test_list[i] = pd.DataFrame(mm_scaler.transform(X_test_list[i]), columns=INPUT_COLUMN)\n","\n","    sub_data_dict['X_train'] = np.array(X_train_list)\n","    sub_data_dict['y_train'] = np.array(y_train_list)\n","    sub_data_dict['y_train_time'] = np.array(y_train_time_list)\n","    sub_data_dict['X_validation'] = np.array(X_validation_list)\n","    sub_data_dict['y_validation'] = np.array(y_validation_list)\n","    sub_data_dict['y_validation_time'] = np.array(y_validation_time_list)\n","    sub_data_dict['X_test'] = np.array(X_test_list)\n","    sub_data_dict['y_test'] = np.array(y_test_list)\n","    sub_data_dict['y_test_time'] = np.array(y_test_time_list)\n","    data_preprocessed_dict[key] = sub_data_dict\n","\n","    print(f\"{key} 행정구역에 대한 처리 결과\" + \"=\"*30)\n","    print(\"train input shape :\", sub_data_dict['X_train'].shape)\n","    print(\"train label shape :\", sub_data_dict['y_train'].shape)\n","    print(\"train time shape :\", sub_data_dict['y_train_time'].shape)\n","    print(\"validation input shape :\", sub_data_dict['X_validation'].shape)\n","    print(\"validation label shape :\", sub_data_dict['y_validation'].shape)\n","    print(\"validation time shape :\", sub_data_dict['y_validation_time'].shape)\n","    print(\"test input shape :\", sub_data_dict['X_test'].shape)\n","    print(\"test label shape :\", sub_data_dict['y_test'].shape)\n","    print(\"test time shape :\", sub_data_dict['y_test_time'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJLkidZCZQfV","executionInfo":{"status":"ok","timestamp":1703863427126,"user_tz":-540,"elapsed":882,"user":{"displayName":"민재홍","userId":"10669517274837738962"}},"outputId":"409f9b23-57ab-4dc7-a43b-c071b3e2cc0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SIG_CD_28110 행정구역에 대한 처리 결과==============================\n","train input shape : (108, 6, 6)\n","train label shape : (108, 1)\n","train time shape : (108, 1)\n","validation input shape : (12, 6, 6)\n","validation label shape : (12, 1)\n","validation time shape : (12, 1)\n","test input shape : (30, 6, 6)\n","test label shape : (30, 1)\n","test time shape : (30, 1)\n"]}]},{"cell_type":"markdown","source":["## LSTM optimization process\n","- 아래 과정을 모든 행정구역 데이터에 대해 반복\n","    - bayesian optimization을 통해 각 행정구역별 LSTM 최적화"],"metadata":{"id":"cH9TrBBsmWWk"}},{"cell_type":"code","source":["total_result = pd.DataFrame()\n","total_feature_importance = pd.DataFrame()\n","for key in data_preprocessed_dict:\n","\n","    X_train = data_preprocessed_dict[key]['X_train']\n","    y_train = data_preprocessed_dict[key]['y_train']\n","    X_validation = data_preprocessed_dict[key]['X_validation']\n","    y_validation = data_preprocessed_dict[key]['y_validation']\n","    X_test = data_preprocessed_dict[key]['X_test']\n","    y_test = data_preprocessed_dict[key]['y_test']\n","    time_train = data_preprocessed_dict[key]['y_train_time']\n","    time_validation = data_preprocessed_dict[key]['y_validation_time']\n","    time_test = data_preprocessed_dict[key]['y_test_time']\n","\n","    # RMSE 정의\n","    def RMSE(y_true, y_pred):\n","        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","\n","    # LSTM 모델 정의\n","    def lstm_model(params):\n","        model = Sequential()\n","        model.add(LSTM(units=int(params['units']),\n","                    activation='tanh',\n","                    recurrent_dropout=params['recurrent_dropout'],\n","                    return_sequences=False,\n","                    input_shape=(SEQ_LEN, X_train.shape[-1])))\n","        model.add(BatchNormalization()),\n","        model.add(Dense(1, activation='linear'))\n","        optimizer = AdamW(learning_rate=params['learning_rate'])\n","        model.compile(optimizer=optimizer, loss=RMSE)\n","        return model\n","\n","    # 목적 함수 정의\n","    def objective(params):\n","        model = lstm_model(params)\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, verbose=1, restore_best_weights=True) # PATIENCE는 현재 30번으로 되어 있으며 30번 학습 뒤로 가장 낮은 loss가 나오지 않으면 조기종료\n","        history = model.fit(X_train, y_train,\n","                            epochs=EPOCHS,\n","                            batch_size=int(params['batch_size']),\n","                            shuffle=False,\n","                            validation_data=(X_validation, y_validation),\n","                            callbacks=[early_stopping],\n","                            verbose=1)\n","\n","        # Early stopping이 발생한 시점에서의 validation RMSE를 반환\n","        val_rmse = early_stopping.best\n","        return val_rmse\n","\n","    # 하이퍼파라미터 탐색 범위 설정(아래 값들을 원하는 값으로 수정하시면 됩니다)\n","    space = {\n","        'units': hp.quniform('units', 32, 256, 16), # LSTM 노드 수를 32개부터 256개까지 16개씩 증가하여 서치\n","        'recurrent_dropout': hp.uniform('recurrent_dropout', 0, 0.5), # recurrent_dropout 비율 0부터 0.5까지 연속적인 범위에서 무작위로 선택\n","        'learning_rate': hp.uniform('learning_rate', 0.0005, 0.05), # learning_rate 0.0005부터 0.1 선택\n","        'batch_size': hp.quniform('batch_size', 4, 64, 4), # LSTM 노드 수를 4개부터 64개까지 4개씩 증가하여 서치\n","    }\n","\n","    # Bayesian optimization 실행(최대 실험 횟수MAX_EVALS는 위에서 30개로 설정되어 있으나, 늘려도 됩니다. 단, 시간이 더 오래 걸립니다.)\n","    print(\"최적 하이퍼파라미터 추출을 위한 작업을 진행합니다.\")\n","    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS)\n","\n","    # 최적 하이퍼파라미터 출력\n","    print(\"최적 하이퍼파라미터:\", best)\n","\n","    # 최적 모델 재학습\n","    print(\"최적 하이퍼파라미터로 재학습 진행합니다.\")\n","    best_model = lstm_model(best)\n","    history =  best_model.fit(X_train, y_train,\n","                epochs=EPOCHS,\n","                batch_size=int(best['batch_size']),\n","                validation_data=(X_validation, y_validation))\n","\n","    # 최적 모델 저장 경로 설정\n","    model_save_folder = f'{MODEL_PATH}{key}/'\n","    os.makedirs(model_save_folder, exist_ok=True) # model 저장 폴더가 없으면 생성\n","    model_save_path = f'{model_save_folder}best_model.h5'\n","\n","    # 최적 모델 저장\n","    print(\"최적 모델을 저장합니다.\")\n","    save_model(best_model, model_save_path)\n","\n","    # 그래프 관련 저장 경로 설정\n","    graph_save_folder = f'{GRAPH_PATH}{key}/'\n","    os.makedirs(graph_save_folder, exist_ok=True) # model 저장 폴더가 없으면 생성\n","    loss_graph_save_path = f'{graph_save_folder}loss_graph.png'\n","    feature_importance_graph_save_path = f'{graph_save_folder}feature_importance_graph.png'\n","    total_graph_save_path = f'{graph_save_folder}total_graph.png'\n","    train_graph_save_path = f'{graph_save_folder}train_graph.png'\n","    validation_graph_save_path = f'{graph_save_folder}validation_graph.png'\n","    test_graph_save_path = f'{graph_save_folder}test_graph.png'\n","\n","    # epoch별 train_loss와 validation_loss에 대한 그래프 작성\n","    print(\"Loss graph를 작성합니다\")\n","    plt.plot(history.history['loss'], label='Train Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(loss_graph_save_path)\n","    plt.show()\n","\n","    # 모델을 사용하여 예측\n","    print(\"train, validation, test set에 대한 예측을 진행합니다.\")\n","    y_train_pred = best_model.predict(X_train)\n","    y_validation_pred = best_model.predict(X_validation)\n","    y_test_pred = best_model.predict(X_test)\n","\n","    df_train_graph = pd.DataFrame({'time':time_train.reshape(-1), 'HOUSING_E_actual':y_train.reshape(-1), 'HOUSING_E_pred':y_train_pred.reshape(-1), 'type':'train'})\n","    df_validation_graph = pd.DataFrame({'time':time_validation.reshape(-1), 'HOUSING_E_actual':y_validation.reshape(-1), 'HOUSING_E_pred':y_validation_pred.reshape(-1), 'type':'validation'})\n","    df_test_graph = pd.DataFrame({'time':time_test.reshape(-1), 'HOUSING_E_actual':y_test.reshape(-1), 'HOUSING_E_pred':y_test_pred.reshape(-1), 'type':'test'})\n","    df_graph = pd.concat([df_train_graph,df_validation_graph,df_test_graph],axis=0).reset_index(drop=True)\n","    df_graph['time'] = pd.to_datetime(df_graph['time'], format=\"%Y%m\")\n","    df_graph['time'] = df_graph['time'].dt.strftime('%Y-%m')\n","\n","    # 2021년 01월 예측\n","    print(\"2021년 01월에 대한 예측을 진행합니다.\")\n","    tmp = data_dict[key].iloc[-SEQ_LEN:,:]\n","    tmp = tmp[INPUT_COLUMN]\n","    tmp = mm_scaler.transform(tmp)\n","    tmp = np.expand_dims(tmp,0)\n","    prediction = best_model.predict(tmp)[0][0]\n","\n","    # 각 지표 측정\n","    result_tmp = {}\n","    result_tmp['ID'] = key\n","    result_tmp['train_RMSE'] = [np.sqrt(mean_squared_error(y_train, y_train_pred))]\n","    result_tmp['train_MSE'] = [mean_squared_error(y_train, y_train_pred)]\n","    result_tmp['train_MAE'] = [mean_absolute_error(y_train, y_train_pred)]\n","    result_tmp['train_MAPE'] = [np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100]\n","    result_tmp['validation_RMSE'] = [np.sqrt(mean_squared_error(y_validation, y_validation_pred))]\n","    result_tmp['validation_MSE'] = [mean_squared_error(y_validation, y_validation_pred)]\n","    result_tmp['validation_MAE'] = [mean_absolute_error(y_validation, y_validation_pred)]\n","    result_tmp['validation_MAPE'] = [np.mean(np.abs((y_validation - y_validation_pred) / y_validation)) * 100]\n","    result_tmp['test_RMSE'] = [np.sqrt(mean_squared_error(y_test, y_test_pred))]\n","    result_tmp['test_MSE'] = [mean_squared_error(y_test, y_test_pred)]\n","    result_tmp['test_MAE'] = [mean_absolute_error(y_test, y_test_pred)]\n","    result_tmp['test_MAPE'] = [np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100]\n","    result_tmp['next_month_HOUSING_E'] = [prediction]\n","    result_tmp = pd.DataFrame(result_tmp)\n","    total_result = pd.concat([total_result, result_tmp], axis=0)\n","\n","    # 변수 중요도 추출\n","    print(\"변수 중요도를 계산합니다\")\n","    print(\"Original Mean Squared Error:\", result_tmp['test_MSE'])\n","\n","    feature_importance = {}\n","    feature_importance['ID'] = key\n","    for i in range(len(INPUT_COLUMN)):\n","        X_permuted = X_test.copy()\n","        X_permuted[:,:,i] = shuffle(X_permuted[:,:,i])\n","\n","        y_pred_permuted = best_model.predict(X_permuted)\n","        mse_permuted = mean_squared_error(y_test, y_pred_permuted)\n","\n","        feature_importance[INPUT_COLUMN[i]] = np.abs(result_tmp['test_MSE'][0] - mse_permuted)\n","\n","    # 중요도 출력\n","    print(\"변수 중요도를 출력합니다\")\n","    print(\"Permutation Feature Importance:\", feature_importance)\n","    plt.barh(list(feature_importance.keys())[1:], list(feature_importance.keys())[1:])\n","    plt.xlabel('Permutation Feature Importance')\n","    plt.ylabel('Features')\n","    plt.title('Permutation Feature Importance for LSTM Model')\n","    plt.savefig(feature_importance_graph_save_path)\n","    plt.show()\n","\n","    feature_importance = pd.DataFrame.from_dict(feature_importance, orient='index', columns=['Permutation Feature Importance'])\n","    feature_importance = feature_importance.T\n","    feature_importance.index = [0]\n","    total_feature_importance = pd.concat([total_feature_importance, feature_importance], axis=0)\n","\n","    # 전체 예측 그래프\n","    print(\"전체 데이터에 대한 실제값과 예측값 비교 그래프를 작성합니다\")\n","    ax1 = sns.lineplot(x='time', y='HOUSING_E_actual', data=df_graph)\n","    ax1 = sns.lineplot(x='time', y='HOUSING_E_pred', data=df_graph)\n","    ax1.set_xticks(np.arange(0, len(df_graph)+1,12))\n","    plt.xticks(rotation=45)\n","    plt.title('Total actual vs predict')\n","    plt.xlabel('Time')\n","    plt.ylabel('HOUSING_E')\n","    plt.legend()\n","    plt.savefig(total_graph_save_path)\n","    plt.show()\n","\n","    # train 예측 그래프\n","    print(\"train set에 대한 실제값과 예측값 비교 그래프를 작성합니다\")\n","    ax2 = sns.lineplot(x='time', y='HOUSING_E_actual', data=df_graph.loc[df_graph['type']=='train'])\n","    ax2 = sns.lineplot(x='time', y='HOUSING_E_pred', data=df_graph.loc[df_graph['type']=='train'])\n","    ax2.set_xticks(np.arange(0, len(df_graph.loc[df_graph['type']=='train'])+1,12))\n","    plt.xticks(rotation=45)\n","    plt.title('Train actual vs predict')\n","    plt.xlabel('Time')\n","    plt.ylabel('HOUSING_E')\n","    plt.legend()\n","    plt.savefig(train_graph_save_path)\n","    plt.show()\n","\n","    # validaiton 예측 그래프\n","    print(\"validaiton set에 대한 실제값과 예측값 비교 그래프를 작성합니다\")\n","    ax3 = sns.lineplot(x='time', y='HOUSING_E_actual', data=df_graph.loc[df_graph['type']=='validation'])\n","    ax3 = sns.lineplot(x='time', y='HOUSING_E_pred', data=df_graph.loc[df_graph['type']=='validation'])\n","    ax3.set_xticks(np.arange(0, len(df_graph.loc[df_graph['type']=='validation'])+1,3))\n","    plt.xticks(rotation=45)\n","    plt.title('Validation actual vs predict')\n","    plt.xlabel('Time')\n","    plt.ylabel('HOUSING_E')\n","    plt.legend()\n","    plt.savefig(validation_graph_save_path)\n","    plt.show()\n","\n","    # test 예측 그래프\n","    print(\"test set에 대한 실제값과 예측값 비교 그래프를 작성합니다\")\n","    ax4 = sns.lineplot(x='time', y='HOUSING_E_actual', data=df_graph.loc[df_graph['type']=='test'])\n","    ax4 = sns.lineplot(x='time', y='HOUSING_E_pred', data=df_graph.loc[df_graph['type']=='test'])\n","    ax4.set_xticks(np.arange(0, len(df_graph.loc[df_graph['type']=='test'])+1,3))\n","    plt.xticks(rotation=45)\n","    plt.title('Test actual vs predict')\n","    plt.xlabel('Time')\n","    plt.ylabel('HOUSING_E')\n","    plt.legend()\n","    plt.savefig(test_graph_save_path)\n","    plt.show()"],"metadata":{"id":"4K1xbuNhlHcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최종 metric 결과 및 feature importance 결과 저장\n","os.makedirs(METRIC_RESULT_PATH, exist_ok=True) # model 저장 폴더가 없으면 생성\n","\n","print(\"각 행정구역에 대한 feature importance 결과를 저장합니다.\")\n","total_feature_importance.to_csv(METRIC_RESULT_PATH+\"feature_importance.csv\")\n","\n","print(\"각 행정구역에 대한 최종 metric 결과를 저장합니다.\")\n","total_result.to_csv(METRIC_RESULT_PATH+\"total_result.csv\")"],"metadata":{"id":"2yWZJnKSl_z2"},"execution_count":null,"outputs":[]}]}